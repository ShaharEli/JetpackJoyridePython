import random
import pygame
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
import logging

# Constants
WIDTH = 1000
HEIGHT = 600
FPS = 60
INIT_Y = HEIGHT - 130
GRAVITY = 0.4

# Initialize pygame
pygame.init()
screen = pygame.display.set_mode([WIDTH, HEIGHT])
surface = pygame.Surface((WIDTH, HEIGHT), pygame.SRCALPHA)
pygame.display.set_caption("Jetpack Joyride Remake in Python!")
font = pygame.font.Font("freesansbold.ttf", 32)
timer = pygame.time.Clock()

# Set up logging
logging.basicConfig(filename='jetpack_joyride_ppo.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)

class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, state):
        return self.network(state)

class PPOMemory:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []

    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.log_probs.clear()
        self.dones.clear()

    def add(self, state, action, reward, value, log_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)

class PPOAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim)
        self.optimizer = optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()), lr=0.0003)
        self.memory = PPOMemory()
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_epsilon = 0.2
        self.epochs = 10

    def choose_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.actor(state)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        value = self.critic(state)
        return action.item(), log_prob.item(), value.item()

    def update(self):
        states = torch.FloatTensor(self.memory.states)
        actions = torch.LongTensor(self.memory.actions)
        rewards = torch.FloatTensor(self.memory.rewards)
        values = torch.FloatTensor(self.memory.values)
        log_probs = torch.FloatTensor(self.memory.log_probs)
        dones = torch.FloatTensor(self.memory.dones)

        # Compute advantages
        advantages = torch.zeros_like(rewards)
        last_gae_lam = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = last_gae_lam = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_gae_lam

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        for _ in range(self.epochs):
            # Compute actor loss
            new_action_probs = self.actor(states)
            new_dist = Categorical(new_action_probs)
            new_log_probs = new_dist.log_prob(actions)
            ratio = torch.exp(new_log_probs - log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()

            # Compute critic loss
            new_values = self.critic(states).squeeze()
            returns = advantages + values
            critic_loss = nn.MSELoss()(new_values, returns)

            # Compute total loss
            loss = actor_loss + 0.5 * critic_loss

            # Optimize the model
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        self.memory.clear()

    def save(self, filename):
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, filename)

    def load(self, filename):
        checkpoint = torch.load(filename)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

class Game:
    def __init__(self, seed=None):
        self.game_speed = 3
        self.player = Player()
        self.laser = Laser()
        self.rocket = Rocket(self.player)
        self.ui = UI()
        self.seed = seed
        self.state_dim = 9
        self.action_dim = 2
        self.agent = PPOAgent(self.state_dim, self.action_dim)
        self.best_reward = float('-inf')

        if self.seed is not None:
            self.set_seed(self.seed)

        self.load_agent()

    def set_seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        pygame.init()
        pygame.display.set_mode([WIDTH, HEIGHT])
        pygame.display.set_caption("Jetpack Joyride Remake in Python!")
        logging.info(f"Game seed set to: {seed}")

    def get_state(self):
        if self.laser.lasers:
            lasers = sorted(self.laser.lasers, key=lambda x: x[0][0])
            lasers = list(filter(lambda x: x[0][0] >= 100, lasers))
            if len(lasers) > 1:
                laser1, laser2 = lasers[:2]
            else:
                laser1 = laser2 = lasers[0]
        else:
            laser1 = [[WIDTH, 0], [WIDTH, HEIGHT]]
            laser2 = laser1

        missile_up = self.rocket.coords[1] if self.rocket.active else HEIGHT * 2
        missile_horizontal = self.rocket.coords[0] if self.rocket.active else WIDTH * 2
        
        return [
            self.player.y,
            self.player.y_velocity,
            self.game_speed,
            laser1[0][1],
            laser1[1][1],
            laser2[0][1],
            laser2[1][1],
            missile_up,
            missile_horizontal,
        ]

    def update(self):
        state = self.get_state()
        action, log_prob, value = self.agent.choose_action(state)

        self.player.booster = action == 1
        self.player.update(GRAVITY)
        self.laser.update(self.game_speed)
        self.rocket.update(self.game_speed)
        self.ui.update(self.game_speed)

        # Check for collisions
        player_rect = self.player.get_collision_rect()
        collision = self.laser.check_collision(player_rect) or self.rocket.check_collision(player_rect)

        reward = 1 if not collision else -100
        done = collision

        next_state = self.get_state()
        self.agent.memory.add(state, action, reward, value, log_prob, done)

        if done:
            self.agent.update()
            self.save_agent()
            logging.info(f"Game Over. Distance: {self.ui.distance:.2f}, Best Reward: {self.best_reward:.2f}")
            self.reset()

        return done

    def reset(self):
        self.player = Player()
        self.laser = Laser()
        self.rocket = Rocket(self.player)
        self.game_speed = 3
        self.ui.distance = 0

    def draw(self):
        screen.fill("black")
        self.ui.draw_bg()
        self.laser.draw()
        self.player.draw()
        self.rocket.draw()
        self.ui.draw_score()
        pygame.display.flip()

    def run(self):
        running = True
        while running:
            timer.tick(FPS)
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False

            done = self.update()
            self.draw()

            if done:
                self.best_reward = max(self.best_reward, self.ui.distance)

        pygame.quit()

    def save_agent(self):
        self.agent.save("ppo_agent.pth")
        logging.info("Agent saved")

    def load_agent(self):
        try:
            self.agent.load("ppo_agent.pth")
            logging.info("Agent loaded")
        except FileNotFoundError:
            logging.info("No saved agent found. Starting with a new agent.")

if __name__ == "__main__":
    game = Game(seed=42)
    game.run()